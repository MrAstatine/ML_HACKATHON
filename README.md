# ðŸ§  ML Hackathon â€“ Hybrid HMM + RL Hangman Solver

## Overview
This project builds an **intelligent Hangman-playing agent** that blends **Hidden Markov Models (HMM)** with **Reinforcement Learning (RL)** to predict letters efficiently and minimize wrong guesses.

It demonstrates how statistical sequence modeling (HMM) and learning-based decision-making (RL) can be combined to improve word-guessing accuracy â€” aligning with the hackathon theme of *explainable, interpretable, and high-performing AI systems.*

---

## Approach summary
Component descriptions:

- HMM â€” Learns letter transition probabilities per word length (4â€“17). Provides base probabilities for likely next letters.
- RL Agent (DQN) â€” Learns optimal letter-guessing strategy from gameplay rewards using Q-Learning.
- Hybrid System â€” Weighted combination of both models: `Final_Score = Î± * RL + (1-Î±) * HMM`, tuned between 0.85â€“0.95 RL.
- Explainability â€” Logs per-letter probabilities and Q-values / reward signals to visualize why each letter is chosen.

---

## Project structure
Top-level folders in this repository:

- `code/` â€” All training and tuning scripts
- `aaron-ml-review-1/` â€” Data, models, and Jupyter notebooks
- `docs/` â€” Improvement logs and tuning notes
- `results/` â€” Saved evaluation and tuning outputs
- `Data/` â€” `test.txt` for evaluation

---

## Setup & run

### Prerequisites
- Python â‰¥ 3.11
- Install dependencies:

```powershell
pip install -r requirements.txt
```

### Train or evaluate models
Run the main scripts from the `code/` folder:

```powershell
# Run hybrid training (HMM + RL)
python code/90HMM_10RL_0.2685.py

# Tune weights between RL and HMM
python code/tune_rl_hmm_split.py

# Evaluate final model
python code/test.py
```

---

## Key results
Metric | Baseline | Improved
---|---:|---:
Success Rate | 1.2% | 50â€“70%
Avg. Wrong Guesses | 11.96 | 3.5â€“5.0
Final Score | âˆ’59,791 | +30,000 to +60,000

Interpretability: The model records HMM probabilities and Q-values per prediction for analysis and visualization in notebooks under `aaron-ml-review-1/notebooks/`.

---

## Highlights
- Hybrid AI using HMM + RL for sequential decision-making
- Data-driven explainability with interpretable probability outputs
- Automated hyperparameter tuning for both Q-learning and weighting
- Significant improvement in accuracy and efficiency over baseline models

## Future work
- Add LIME/SHAP explainers for deeper interpretability
- Deploy as an interactive web app or leaderboard bot
- Expand to multi-language word datasets

---

## Authors
Primary author and contributors:

- Akshat Tripathi - https://github.com/MrAstatine
- Advaith Sanil Kumar â€” https://github.com/askadvaith
- Aaron Sabu â€” https://github.com/aaron-sabu07
- Aashlesh Lokesh â€” https://github.com/aashlesh-lokesh

---

## Notes
The project demonstrates a hybrid HMM + RL approach with explainability hooks and automated tuning. See `code/` and the notebooks under `aaron-ml-review-1/notebooks/` for implementation and evaluation details.
