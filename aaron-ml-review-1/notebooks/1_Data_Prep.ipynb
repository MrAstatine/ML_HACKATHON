{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPdjJ7xp7dzSrr22Wp/6Aoy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Notebook 1: Data Preprocessing & Setup\n","\n"],"metadata":{"id":"t0DSD-AhwkWK"}},{"cell_type":"markdown","source":["### Mount Google Drive"],"metadata":{"id":"js1Uv6g5wz9s"}},{"cell_type":"code","execution_count":16,"metadata":{"id":"Pos99IPLwfal","executionInfo":{"status":"ok","timestamp":1762141658211,"user_tz":-330,"elapsed":34529,"user":{"displayName":"Aaron Sabu","userId":"06047287740762101122"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1e912d5e-883b-4f30-b4f4-655e2ef577e7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["### Import Libraries"],"metadata":{"id":"g_ybhJyiw4Oc"}},{"cell_type":"code","source":["import string\n","import pprint  # For clean dictionary printing\n","from google.colab import files\n","import io      # To handle file content\n","import os      # To create directories\n","import json    # To save our processed data"],"metadata":{"id":"6ph7YS7Ew7Gy","executionInfo":{"status":"ok","timestamp":1762141660283,"user_tz":-330,"elapsed":16,"user":{"displayName":"Aaron Sabu","userId":"06047287740762101122"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["### Setup Project Directory"],"metadata":{"id":"JMizKJWcxHRW"}},{"cell_type":"code","source":["# --- Configuration ---\n","BASE_PATH = '/content/drive/My Drive/ml-hackathon'\n","\n","# Define our project structure\n","DATA_PATH = os.path.join(BASE_PATH, 'data')\n","MODEL_PATH = os.path.join(BASE_PATH, 'models')\n","NOTEBOOK_PATH = os.path.join(BASE_PATH, 'notebooks')\n","REPORT_PATH = os.path.join(BASE_PATH, 'reports')\n","\n","# Define paths for our processed files\n","CORPUS_JSON_PATH = os.path.join(DATA_PATH, 'corpus_by_length.json')\n","TEST_JSON_PATH = os.path.join(DATA_PATH, 'test_by_length.json')\n","CORPUS_SET_PATH = os.path.join(DATA_PATH, 'all_corpus_words_set.json')\n","\n","# --- Create Directories ---\n","print(f\"Base project path set to: {BASE_PATH}\")\n","print(\"Creating project directories (if they don't exist)...\")\n","\n","os.makedirs(DATA_PATH, exist_ok=True)\n","os.makedirs(MODEL_PATH, exist_ok=True)\n","os.makedirs(NOTEBOOK_PATH, exist_ok=True)\n","os.makedirs(REPORT_PATH, exist_ok=True)\n","\n","print(f\"Created: {DATA_PATH}\")\n","print(f\"Created: {MODEL_PATH}\")\n","print(f\"Created: {NOTEBOOK_PATH}\")\n","print(f\"Created: {REPORT_PATH}\")\n","print(\"\\nDirectory setup complete.\")"],"metadata":{"id":"awHtXlXNxSto","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762141741641,"user_tz":-330,"elapsed":1640,"user":{"displayName":"Aaron Sabu","userId":"06047287740762101122"}},"outputId":"278fc28f-6961-4fcf-fafd-c34d05f251df"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Base project path set to: /content/drive/My Drive/ml-hackathon\n","Creating project directories (if they don't exist)...\n","Created: /content/drive/My Drive/ml-hackathon/data\n","Created: /content/drive/My Drive/ml-hackathon/models\n","Created: /content/drive/My Drive/ml-hackathon/notebooks\n","Created: /content/drive/My Drive/ml-hackathon/reports\n","\n","Directory setup complete.\n"]}]},{"cell_type":"markdown","source":["### Upload Raw Data"],"metadata":{"id":"9FISYRqLxv-Z"}},{"cell_type":"code","source":["print(\"Please upload 'corpus.txt' and 'test.txt'\")\n","uploaded_files = files.upload()\n","\n","# --- Robustly find and store content ---\n","corpus_content = None\n","test_content = None\n","\n","# Loop through all uploaded files and find the right ones\n","for filename, content in uploaded_files.items():\n","    if 'corpus' in filename.lower():\n","        corpus_content = content.decode('utf-8')\n","        print(f\"Found and decoded corpus file: {filename}\")\n","    elif 'test' in filename.lower():\n","        test_content = content.decode('utf-8')\n","        print(f\"Found and decoded test file: {filename}\")\n","\n","# --- Check if we got both ---\n","if corpus_content and test_content:\n","    print(\"\\nSuccessfully uploaded and decoded both files.\")\n","else:\n","    print(\"\\nError: One or both files were not found. Please re-run this cell and upload 'corpus.txt' and 'test.txt'.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":194},"id":"z6BkGRwUx40Q","executionInfo":{"status":"ok","timestamp":1762141809032,"user_tz":-330,"elapsed":12913,"user":{"displayName":"Aaron Sabu","userId":"06047287740762101122"}},"outputId":"0100cc5d-73cb-47c4-ed4f-0fdcd1a5a604"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Please upload 'corpus.txt' and 'test.txt'\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-e14ef784-276a-4f5b-9186-a3cf126e7a37\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-e14ef784-276a-4f5b-9186-a3cf126e7a37\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving corpus.txt to corpus (7).txt\n","Saving test.txt to test (6).txt\n","Found and decoded corpus file: corpus (7).txt\n","Found and decoded test file: test (6).txt\n","\n","Successfully uploaded and decoded both files.\n"]}]},{"cell_type":"markdown","source":["### Helper Functions"],"metadata":{"id":"Z25MhXyE1mC8"}},{"cell_type":"code","source":["def load_words_from_content(content):\n","    \"\"\"Reads file content, returning a list of cleaned, uppercase words.\"\"\"\n","    words = []\n","    # .splitlines() splits the content by new lines\n","    for line in content.splitlines():\n","        # .strip() removes whitespace\n","        # .upper() converts to uppercase\n","        word = line.strip().upper()\n","\n","        # Ensure word is not empty and contains only standard A-Z letters\n","        if word and all(c in string.ascii_uppercase for c in word):\n","            words.append(word)\n","    return words\n","\n","def group_by_length(words):\n","    \"\"\"Groups a list of words into a dictionary by their length.\"\"\"\n","    words_by_length = {}\n","    for word in words:\n","        length = len(word)\n","        if length not in words_by_length:\n","            words_by_length[length] = []\n","        words_by_length[length].append(word)\n","    return words_by_length"],"metadata":{"id":"NBPxMaz81ndU","executionInfo":{"status":"ok","timestamp":1762141883922,"user_tz":-330,"elapsed":31,"user":{"displayName":"Aaron Sabu","userId":"06047287740762101122"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["### Process Data and Save to Drive"],"metadata":{"id":"o2Zx2uOi17MD"}},{"cell_type":"code","source":["print(\"Processing and structuring data...\")\n","\n","# 1. Process the training corpus\n","corpus_words = load_words_from_content(corpus_content)\n","corpus_by_length = group_by_length(corpus_words)\n","all_corpus_words_set = set(corpus_words)\n","print(f\"Successfully processed {len(corpus_words)} corpus words.\")\n","\n","# 2. Process the test set\n","test_words = load_words_from_content(test_content)\n","test_by_length = group_by_length(test_words)\n","print(f\"Successfully processed {len(test_words)} test words.\")\n","\n","# 3. Save processed files to Google Drive\n","print(\"\\nSaving processed files to Google Drive...\")\n","\n","# Save corpus grouped by length\n","with open(CORPUS_JSON_PATH, 'w') as f:\n","    json.dump(corpus_by_length, f)\n","print(f\"Saved corpus (grouped by length) to: {CORPUS_JSON_PATH}\")\n","\n","# Save test set grouped by length\n","with open(TEST_JSON_PATH, 'w') as f:\n","    json.dump(test_by_length, f)\n","print(f\"Saved test set (grouped by length) to: {TEST_JSON_PATH}\")\n","\n","# Save the full corpus word set (as a list for JSON)\n","with open(CORPUS_SET_PATH, 'w') as f:\n","    json.dump(list(all_corpus_words_set), f)\n","print(f\"Saved full corpus word set to: {CORPUS_SET_PATH}\")\n","\n","print(\"\\n--- Corpus Summary (Words per Length) ---\")\n","corpus_lengths = {length: len(words) for length, words in corpus_by_length.items()}\n","pprint.pprint(corpus_lengths)\n","\n","print(\"\\nStep 2: Preprocessing and Saving Complete.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b-bzkKoB1-if","executionInfo":{"status":"ok","timestamp":1762141946680,"user_tz":-330,"elapsed":231,"user":{"displayName":"Aaron Sabu","userId":"06047287740762101122"}},"outputId":"0dec6a16-00c7-477d-9d5f-1b8250a9466f"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing and structuring data...\n","Successfully processed 49979 corpus words.\n","Successfully processed 2000 test words.\n","\n","Saving processed files to Google Drive...\n","Saved corpus (grouped by length) to: /content/drive/My Drive/ml-hackathon/data/corpus_by_length.json\n","Saved test set (grouped by length) to: /content/drive/My Drive/ml-hackathon/data/test_by_length.json\n","Saved full corpus word set to: /content/drive/My Drive/ml-hackathon/data/all_corpus_words_set.json\n","\n","--- Corpus Summary (Words per Length) ---\n","{1: 46,\n"," 2: 84,\n"," 3: 388,\n"," 4: 1169,\n"," 5: 2340,\n"," 6: 3755,\n"," 7: 5111,\n"," 8: 6348,\n"," 9: 6787,\n"," 10: 6465,\n"," 11: 5452,\n"," 12: 4292,\n"," 13: 3094,\n"," 14: 2019,\n"," 15: 1226,\n"," 16: 698,\n"," 17: 375,\n"," 18: 174,\n"," 19: 88,\n"," 20: 40,\n"," 21: 16,\n"," 22: 8,\n"," 23: 3,\n"," 24: 1}\n","\n","Step 2: Preprocessing and Saving Complete.\n"]}]},{"cell_type":"markdown","source":["### Save Sorted Unique Word List"],"metadata":{"id":"cDk_1BAZ4Eq0"}},{"cell_type":"code","source":["print(\"\\nCreating sorted unique word list...\")\n","\n","# Define the new output path\n","SORTED_WORDS_PATH = os.path.join(DATA_PATH, 'corpus_sorted_unique.json') # <-- Output for new cell\n","\n","try:\n","    # 1. Load the unique word set file\n","    with open(CORPUS_SET_PATH, 'r') as f:\n","        # We must load the data from the 'all_corpus_words_set.json' file\n","        unique_words = json.load(f)\n","    print(f\"Loaded {len(unique_words)} unique words from {CORPUS_SET_PATH}\")\n","\n","    # 2. Sort the list alphabetically\n","    unique_words.sort()\n","    print(\"Alphabetically sorted all unique words.\")\n","\n","    # 3. Save the new sorted list to Google Drive\n","    with open(SORTED_WORDS_PATH, 'w') as f:\n","        json.dump(unique_words, f)\n","    print(f\"Successfully saved sorted list to: {SORTED_WORDS_PATH}\")\n","\n","    print(\"\\n--- First 50 words of sorted list ---\")\n","    print(unique_words[:50])\n","\n","except NameError:\n","    print(\"Error: Make sure 'DATA_PATH' and 'CORPUS_SET_PATH' are defined.\")\n","    print(\"Please re-run Cell 3 and Cell 6 first.\")\n","except FileNotFoundError:\n","    print(f\"ERROR: File not found at {CORPUS_SET_PATH}\")\n","    print(\"Please make sure Cell 6 ran correctly.\")\n","except Exception as e:\n","    print(f\"An error occurred: {e}\")\n","\n","print(\"\\nNotebook 1, Step 3: Sorted Word List Generation Complete.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZY8nD6Re4GFq","executionInfo":{"status":"ok","timestamp":1762142554662,"user_tz":-330,"elapsed":125,"user":{"displayName":"Aaron Sabu","userId":"06047287740762101122"}},"outputId":"9acd6a89-1355-4005-de65-00d3b05e7b1f"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Creating sorted unique word list...\n","Loaded 49397 unique words from /content/drive/My Drive/ml-hackathon/data/all_corpus_words_set.json\n","Alphabetically sorted all unique words.\n","Successfully saved sorted list to: /content/drive/My Drive/ml-hackathon/data/corpus_sorted_unique.json\n","\n","--- First 50 words of sorted list ---\n","['A', 'AARON', 'AARONIC', 'AARONITIC', 'ABA', 'ABACUS', 'ABAFF', 'ABALIENATION', 'ABANDONER', 'ABARIS', 'ABARTHROSIS', 'ABAS', 'ABASER', 'ABASHEDNESS', 'ABASSIN', 'ABATABLE', 'ABATISED', 'ABATTOIR', 'ABBASSIDE', 'ABBEYSTEDE', 'ABBIE', 'ABBOTCY', 'ABBOTNULLIUS', 'ABBOTSHIP', 'ABBREVIATOR', 'ABDERIAN', 'ABDEST', 'ABDICANT', 'ABDICATE', 'ABDOMINOHYSTERECTOMY', 'ABDOMINOPOSTERIOR', 'ABDUCE', 'ABDUCT', 'ABDUCTION', 'ABEARANCE', 'ABED', 'ABEL', 'ABELITE', 'ABELMOSCHUS', 'ABENCERRAGES', 'ABERDONIAN', 'ABERIA', 'ABERRANT', 'ABERRATOR', 'ABERROSCOPE', 'ABERUNCATOR', 'ABET', 'ABEVACUATION', 'ABHISEKA', 'ABHOMINABLE']\n","\n","Notebook 1, Step 3: Sorted Word List Generation Complete.\n"]}]}]}