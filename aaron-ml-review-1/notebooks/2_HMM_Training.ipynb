{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP72xGacZMOqObI1xfofNtG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Notebook-2: HMM (Probabilistic Oracle) Training\n","\n"],"metadata":{"id":"123mgV8f5NuD"}},{"cell_type":"markdown","source":["### Mount Drive & Define Paths"],"metadata":{"id":"3W3BJMzE5-z-"}},{"cell_type":"code","source":["import os\n","import json\n","import string\n","from google.colab import drive\n","from collections import defaultdict\n","\n","# --- Mount Drive ---\n","drive.mount('/content/drive')\n","\n","# --- Configuration ---\n","# This MUST be the same path as in your first notebook\n","BASE_PATH = '/content/drive/My Drive/ml-hackathon'\n","\n","DATA_PATH = os.path.join(BASE_PATH, 'data')\n","MODEL_PATH = os.path.join(BASE_PATH, 'models')\n","\n","# --- Input File (from Notebook 1) ---\n","CORPUS_JSON_PATH = os.path.join(DATA_PATH, 'corpus_by_length.json')\n","\n","# --- Output File (Our \"Trained HMM\") ---\n","HMM_MODEL_PATH = os.path.join(MODEL_PATH, 'hmm_probabilities.json')\n","\n","print(f\"Base path set to: {BASE_PATH}\")\n","print(f\"Loading corpus from: {CORPUS_JSON_PATH}\")\n","print(f\"Will save trained model to: {HMM_MODEL_PATH}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NmLNP70t6Aa4","executionInfo":{"status":"ok","timestamp":1762143038798,"user_tz":-330,"elapsed":26628,"user":{"displayName":"Aaron Sabu","userId":"06047287740762101122"}},"outputId":"68fe1213-95b9-4564-8291-903f015681c0"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Base path set to: /content/drive/My Drive/ml-hackathon\n","Loading corpus from: /content/drive/My Drive/ml-hackathon/data/corpus_by_length.json\n","Will save trained model to: /content/drive/My Drive/ml-hackathon/models/hmm_probabilities.json\n"]}]},{"cell_type":"markdown","source":["### Load Processed Corpus"],"metadata":{"id":"vLpcwumO6TUs"}},{"cell_type":"code","source":["# Load the corpus grouped by length\n","try:\n","    with open(CORPUS_JSON_PATH, 'r') as f:\n","        corpus_by_length = json.load(f)\n","    print(\"Successfully loaded corpus_by_length.json.\")\n","\n","    # JSON keys are strings, convert them back to integers\n","    corpus_by_length = {int(k): v for k, v in corpus_by_length.items()}\n","\n","    print(f\"Loaded data for {len(corpus_by_length)} word lengths.\")\n","except FileNotFoundError:\n","    print(f\"ERROR: File not found at {CORPUS_JSON_PATH}\")\n","    print(\"Please make sure Notebook 1 ran correctly and the file exists.\")\n","except Exception as e:\n","    print(f\"An error occurred: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N_Wyx_gz6Ulk","executionInfo":{"status":"ok","timestamp":1762143077507,"user_tz":-330,"elapsed":2973,"user":{"displayName":"Aaron Sabu","userId":"06047287740762101122"}},"outputId":"4bcd2d03-02e5-415b-c500-573491b93816"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully loaded corpus_by_length.json.\n","Loaded data for 24 word lengths.\n"]}]},{"cell_type":"markdown","source":["### HMM Probability Functions"],"metadata":{"id":"lLoQYws26bep"}},{"cell_type":"code","source":["def calculate_probabilities(word_list):\n","    \"\"\"\n","    Calculates unigram and bigram probabilities for a given list of words.\n","    This acts as our simplified HMM.\n","    \"\"\"\n","    # Use defaultdict for easy counting\n","    unigram_counts = defaultdict(int)\n","    bigram_counts = defaultdict(lambda: defaultdict(int))\n","    total_letters = 0\n","\n","    # Use a set of all letters for smoothing\n","    alphabet = set(string.ascii_uppercase)\n","\n","    # 1. Count occurrences\n","    for word in word_list:\n","        # Add a \"start\" token for the first letter's bigram\n","        prev_char = 'START'\n","        for char in word:\n","            unigram_counts[char] += 1\n","            bigram_counts[prev_char][char] += 1\n","            prev_char = char\n","            total_letters += 1\n","\n","    # 2. Calculate Probabilities with Laplace (Add-1) Smoothing\n","    # Smoothing prevents zero-probabilities for unseen pairs\n","\n","    # --- Unigram probabilities ---\n","    unigram_probs = {}\n","    total_unigram_denominator = total_letters + len(alphabet) # Add-1 smoothing\n","    for char in alphabet:\n","        unigram_probs[char] = (unigram_counts[char] + 1) / total_unigram_denominator\n","\n","    # --- Bigram probabilities ---\n","    bigram_probs = {}\n","    # We need to calculate for 'START' token + all letters\n","    possible_prev_chars = list(alphabet) + ['START']\n","\n","    for prev_char in possible_prev_chars:\n","        bigram_probs[prev_char] = {}\n","        total_bigram_denominator = sum(bigram_counts[prev_char].values()) + len(alphabet)\n","\n","        for char in alphabet:\n","            bigram_probs[prev_char][char] = (bigram_counts[prev_char][char] + 1) / total_bigram_denominator\n","\n","    return {'unigram': unigram_probs, 'bigram': bigram_probs}"],"metadata":{"id":"VkDi9rUk6dEt","executionInfo":{"status":"ok","timestamp":1762143288153,"user_tz":-330,"elapsed":105,"user":{"displayName":"Aaron Sabu","userId":"06047287740762101122"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["### Train \"HMM\" for Each Word Length"],"metadata":{"id":"nv7W5ZA87QmX"}},{"cell_type":"code","source":["print(\"Training HMM (Probability Models) for each word length...\")\n","\n","# This will store all our models: {5: model_for_5, 6: model_for_6, ...}\n","hmm_models = {}\n","\n","# Get all lengths from our loaded corpus\n","word_lengths = sorted(corpus_by_length.keys())\n","\n","for length in word_lengths:\n","    words = corpus_by_length[length]\n","    if len(words) > 0: # Only train if we have words of that length\n","        hmm_models[length] = calculate_probabilities(words)\n","        print(f\" - Trained model for length {length} (based on {len(words)} words)\")\n","\n","print(f\"\\nTraining complete. Total models trained: {len(hmm_models)}\")\n","\n","# --- Save the combined model to Google Drive ---\n","try:\n","    with open(HMM_MODEL_PATH, 'w') as f:\n","        # We need to convert integer keys to strings for JSON\n","        json.dump({str(k): v for k, v in hmm_models.items()}, f)\n","    print(f\"\\nSuccessfully saved all HMM models to: {HMM_MODEL_PATH}\")\n","except Exception as e:\n","    print(f\"\\nError saving model to Google Drive: {e}\")\n","\n","print(\"\\nNotebook 2: HMM Model Training and Saving Complete.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MR7NTQaW7SkD","executionInfo":{"status":"ok","timestamp":1762143337130,"user_tz":-330,"elapsed":344,"user":{"displayName":"Aaron Sabu","userId":"06047287740762101122"}},"outputId":"7850afed-f265-4054-c105-f6551e1a77be"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Training HMM (Probability Models) for each word length...\n"," - Trained model for length 1 (based on 46 words)\n"," - Trained model for length 2 (based on 84 words)\n"," - Trained model for length 3 (based on 388 words)\n"," - Trained model for length 4 (based on 1169 words)\n"," - Trained model for length 5 (based on 2340 words)\n"," - Trained model for length 6 (based on 3755 words)\n"," - Trained model for length 7 (based on 5111 words)\n"," - Trained model for length 8 (based on 6348 words)\n"," - Trained model for length 9 (based on 6787 words)\n"," - Trained model for length 10 (based on 6465 words)\n"," - Trained model for length 11 (based on 5452 words)\n"," - Trained model for length 12 (based on 4292 words)\n"," - Trained model for length 13 (based on 3094 words)\n"," - Trained model for length 14 (based on 2019 words)\n"," - Trained model for length 15 (based on 1226 words)\n"," - Trained model for length 16 (based on 698 words)\n"," - Trained model for length 17 (based on 375 words)\n"," - Trained model for length 18 (based on 174 words)\n"," - Trained model for length 19 (based on 88 words)\n"," - Trained model for length 20 (based on 40 words)\n"," - Trained model for length 21 (based on 16 words)\n"," - Trained model for length 22 (based on 8 words)\n"," - Trained model for length 23 (based on 3 words)\n"," - Trained model for length 24 (based on 1 words)\n","\n","Training complete. Total models trained: 24\n","\n","Successfully saved all HMM models to: /content/drive/My Drive/ml-hackathon/models/hmm_probabilities.json\n","\n","Notebook 2: HMM Model Training and Saving Complete.\n"]}]}]}